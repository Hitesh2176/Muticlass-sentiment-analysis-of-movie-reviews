# -*- coding: utf-8 -*-
"""assgn2_hitesh.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RSOxSq57SQtIwLFcG8nlyAxSIqh97u59
"""

import csv
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS
from sklearn import metrics


from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from keras import backend as K
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Dense,Dropout,Flatten
from keras.layers import Activation, Conv1D, GlobalMaxPooling1D
from keras import optimizers
from keras.utils import to_categorical

data = pd.read_csv('https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv', sep='\t')
data.head(10)

numSentences = data['SentenceId'].max()

fullSentences = []
curSentence = 0
for i in range(data.shape[0]):
  if data['SentenceId'][i]> curSentence:
    fullSentences.append((data['Phrase'][i], data['Sentiment'][i]))
    curSentence = curSentence +1
print ('num of full sentences in dataset:-',len(fullSentences))

fullSentDf = pd.DataFrame(fullSentences,
                                columns=['Phrase', 'Sentiment'])

import nltk
import random
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.tokenize import word_tokenize

documents = []
for i in range(fullSentDf.shape[0]):
  tmpWords = word_tokenize(fullSentDf['Phrase'][i])
  documents.append((tmpWords, fullSentDf['Sentiment'][i]))
random.seed(9001)
random.shuffle(documents)
print(documents[1][0])
len(documents)

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer
porter = PorterStemmer()
lancaster=LancasterStemmer()
wordnet_lemmatizer = WordNetLemmatizer()
stopwords_en = stopwords.words("english")
punctuations="?:!.,;'\"-()"

#parameters to adjust to see the impact on outcome
remove_stopwords = True
useStemming = True
useLemma = False
removePuncs = True

for l in range(len(documents)):
  label = documents[l][1]
  tmpReview = []
  for w in documents[l][0]:
    newWord = w
    if remove_stopwords and (w in stopwords_en):
      continue
    if removePuncs and (w in punctuations):
      continue
    if useStemming:
      #newWord = porter.stem(newWord)
      newWord = lancaster.stem(newWord)
    if useLemma:
      newWord = wordnet_lemmatizer.lemmatize(newWord)
    tmpReview.append(newWord)
  documents[l] = (' '.join(tmpReview), label)
print(documents[2])

all_data = pd.DataFrame(documents,columns=['text', 'sentiment'])
# Splits the dataset so 70% is used for training and 30% for testing
x_train_raw, x_test_raw, Y_train, Y_test = train_test_split(all_data['text'], all_data['sentiment'], test_size=0.3,random_state=2003)
len(x_train_raw)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Transform each text into a vector of word counts
vectorizer = CountVectorizer(stop_words="english",
                             ngram_range=(1,2))

Y_train = to_categorical(Y_train)
Y_train.shape

train_bow = vectorizer.fit_transform(x_train_raw)
test_bow = vectorizer.transform(x_test_raw)

def recall_m(y_true, y_pred):
  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
  possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
  recall = true_positives / (possible_positives + K.epsilon())
  return recall

def precision_m(y_true, y_pred):
  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
  predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
  precision = true_positives / (predicted_positives + K.epsilon())
  return precision

def f1_m(y_true, y_pred):
  precision = precision_m(y_true,y_pred)
  recall = recall_m(y_true, y_pred)
  return 2*((precision*recall)/(precision+recall+K.epsilon()))

def baseline_cnn_model(fea_matrix, n_class, mode, compiler):
  model = Sequential()
  model.add(Conv1D(filters=64, kernel_size=1, activation='relu',
                  input_shape=(fea_matrix.shape[1],fea_matrix.shape[2])))
  model.add(MaxPooling1D(pool_size=2))
  model.add(Conv1D(filters=128, kernel_size=1, activation='relu'))
  model.add(MaxPooling1D(pool_size=2))
  model.add(Flatten())
  model.add(Activation('relu'))
  model.add(Dense(n_class))
  if n_class == 1 and mode == "cla":
    model.add(Activation('sigmoid'))
    model.compile(optimizer=compiler,loss='binary_crossentropy',
                 metrics=['acc',f1_m,precision_m,recall_m])
  else:
    model.add(Activation('softmax'))
    model.compile(optimizer=compiler,loss='categorical_crossentropy',
                 metrics=['acc',f1_m,precision_m,recall_m])
  return model

try:
  train_bow = np.array(train_bow.toarray())
  train_bow = train_bow.reshape(train_bow.shape[0], train_bow.shape[1], 1)
  test_bow = np.array(test_bow.toarray())
  test_bow = test_bow.reshape(test_bow.shape[0], test_bow.shape[1], 1)
except:
  pass

train_bow.shape

adm = optimizers.Adam(lr=1e-3,decay=1e-4)
sgd = optimizers.SGD(lr=1e-3,nesterov=True,momentum=0.7,decay=1e-4)
Nadam = optimizers.Nadam(lr=1e-3,beta_1=0.9,beta_2=0.999, epsilon=1e-08)
model = baseline_cnn_model(train_bow,5,'cla',adm)

y_train_final = to_categorical(Y_train)
y_test_final = to_categorical(Y_test)


num_epochs=25
for epoch in range(num_epochs):
  print(epoch+1,'/',num_epochs)
  model1=model.fit(train_bow,Y_train,batch_size=128,
         epochs=1,verbose=1,validation_split=0.3)



def print_metrics(accuracy, f1_score,precision,recall):
  print('\n')
  print('Simple CNN model Performance')
  print('Accuracy  : ',np.round(accuracy, 4))
  print('Precision : ',np.round(precision, 4))
  print('Recall    : ',np.round(recall, 4))
  print('F1 Score  : ',np.round(f1_score, 4))
  print('\n')
  
loss, accuracy, f1_score, precision, recall = model.evaluate(test_bow,y_test_final)
print_metrics(accuracy, f1_score,precision,recall)

model.save('sentiment_1114577.pt')